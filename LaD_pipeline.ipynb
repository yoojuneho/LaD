{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e0253b",
   "metadata": {},
   "source": [
    "# 1. chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk.py\n",
    "# Step-1\n",
    "# Generate all possible sentence pairs between [news1, conflict_news, other_news1, other_news2, other_news3]\n",
    "# Sentence pairs within the same document are not generated\n",
    "\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# PATH SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "# Using Path for better cross-platform compatibility\n",
    "BASE_DIR = Path(\"input Low Intensity Factoid Conflict Dataset Folder Path\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "INPUT_PATH = DATA_DIR / \"Low_Intensity_Factoid_Conflict.json\"\n",
    "OUTPUT_DIR = DATA_DIR / \"Chunk\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD spaCy\n",
    "# ============================================================\n",
    "NLP = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Sentence Chunking\n",
    "# ============================================================\n",
    "def chunk_sentences(text, doc_id):\n",
    "    doc = NLP(text)\n",
    "    results = []\n",
    "    for sent in doc.sents:\n",
    "        t = sent.text.strip()\n",
    "        if t:\n",
    "            results.append({\"text\": t, \"doc_id\": doc_id})\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Generate ALL Document Sentence Pairs\n",
    "# ============================================================\n",
    "def generate_all_sentence_pairs(doc_sentences):\n",
    "    \"\"\"\n",
    "    doc_sentences: dict\n",
    "        {\n",
    "            \"news1\": [sent1, sent2, ...],\n",
    "            \"conflict_news\": [sent1, sent2, ...],\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    doc_keys = list(doc_sentences.keys())\n",
    "    pairs = []\n",
    "    pair_id = 1\n",
    "\n",
    "    # All document combinations (without A-B, B-A duplicates)\n",
    "    for i in range(len(doc_keys)):\n",
    "        for j in range(i + 1, len(doc_keys)):\n",
    "            docA = doc_keys[i]\n",
    "            docB = doc_keys[j]\n",
    "\n",
    "            sentsA = doc_sentences[docA]\n",
    "            sentsB = doc_sentences[docB]\n",
    "\n",
    "            # Cartesian product of all sentences\n",
    "            for s1 in sentsA:\n",
    "                for s2 in sentsB:\n",
    "                    pairs.append({\n",
    "                        \"id\": pair_id,\n",
    "                        \"sentence_1\": s1,\n",
    "                        \"sentence_2\": s2,\n",
    "                        \"semantic_score\": \"\",\n",
    "                        \"semantic_filtered\": \"\",\n",
    "                        \"cosine_score\": \"\",\n",
    "                        \"cosine_filtered\": \"\",\n",
    "                        \"conflict_label\": \"\"\n",
    "                    })\n",
    "                    pair_id += 1\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Process Single Event\n",
    "# ============================================================\n",
    "def process_event(event):\n",
    "    event_id = event[\"id\"]\n",
    "    print(f\"=== Processing event {event_id} ===\")\n",
    "\n",
    "    news = event[\"news\"]\n",
    "\n",
    "    # document keys to include in ALL-to-ALL matching\n",
    "    target_keys = [\n",
    "        \"news1\",\n",
    "        \"conflict_news\",\n",
    "        \"other_news1\",\n",
    "        \"other_news2\",\n",
    "        \"other_news3\",\n",
    "    ]\n",
    "\n",
    "    # 1) Sentence chunking for all target documents\n",
    "    doc_sentences = {}\n",
    "    for key in target_keys:\n",
    "        if key in news and \"article\" in news[key]:\n",
    "            doc_sentences[key] = chunk_sentences(news[key][\"article\"], key)\n",
    "\n",
    "    # 2) Generate ALL-to-ALL document sentence pairs\n",
    "    pairs = generate_all_sentence_pairs(doc_sentences)\n",
    "\n",
    "    print(f\"Generated {len(pairs)} sentence pairs (ALL combinations)\")\n",
    "\n",
    "    # Save output\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"event_{event_id}.json\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved → {output_file}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading dataset:\", INPUT_PATH)\n",
    "\n",
    "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for event in data:\n",
    "        process_event(event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0291e4",
   "metadata": {},
   "source": [
    "# 2. Semantic Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_filtering.py\n",
    "# Step-2\n",
    "# Filtering criterion: score > threshold\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# BGE Reranker path\n",
    "BGE_RERANKER = \"bge-reranker-v2-m3\" # Use Hugging Face model name or local path\n",
    "\n",
    "# Path settings\n",
    "BASE_DIR = Path(\"input Low Intensity Factoid Conflict Dataset Folder Path\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"Chunk\"  # Input from chunk.py output\n",
    "OUTPUT_DIR = DATA_DIR / \"Semantic_Filtered\"  # Output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load Model\n",
    "# ============================================================\n",
    "def load_model():\n",
    "    \"\"\"Load BGE Reranker model and tokenizer.\"\"\"\n",
    "    print(f\"Loading Reranker: {BGE_RERANKER}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BGE_RERANKER)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BGE_RERANKER)\n",
    "    model.to(DEVICE).eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compute Semantic Scores\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def compute_semantic_scores(tokenizer, model, pairs, batch_size=16):\n",
    "\n",
    "    q = [p[\"sentence_1\"][\"text\"] for p in pairs]\n",
    "    d = [p[\"sentence_2\"][\"text\"] for p in pairs]\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for start in tqdm(range(0, len(pairs), batch_size), ncols=120):\n",
    "        end = min(start + batch_size, len(pairs))\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            q[start:end],\n",
    "            d[start:end],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        logits = model(**inputs).logits.squeeze(-1).cpu().tolist()\n",
    "        scores.extend(logits)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Gap-based Threshold Filtering\n",
    "# ============================================================\n",
    "def gap_threshold(values):\n",
    "    \"\"\"Find threshold based on the largest drop in score distribution.\"\"\"\n",
    "    sorted_vals = sorted(values, reverse=True)\n",
    "    diffs = np.diff(sorted_vals)\n",
    "    idx = np.argmin(diffs)\n",
    "    return (sorted_vals[idx] + sorted_vals[idx + 1]) / 2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Process Single File\n",
    "# ============================================================\n",
    "def process_file(input_path, output_path, tokenizer, model):\n",
    "    \"\"\"Process a single event file for semantic filtering.\"\"\"\n",
    "    print(\"\\nProcessing:\", input_path)\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        pairs = json.load(f)\n",
    "\n",
    "    # Compute semantic similarity\n",
    "    semantic_scores = compute_semantic_scores(tokenizer, model, pairs)\n",
    "\n",
    "    # Determine threshold\n",
    "    th = gap_threshold(semantic_scores)\n",
    "    print(\"Semantic threshold:\", th)\n",
    "\n",
    "    # Update JSON with scores and filtering results\n",
    "    for p, score in zip(pairs, semantic_scores):\n",
    "        p[\"semantic_score\"] = score\n",
    "        p[\"semantic_filtered\"] = 1 if score > th else 0\n",
    "\n",
    "    # Save to OUTPUT directory\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(pairs, g, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Saved:\", output_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer, model = load_model()\n",
    "\n",
    "    # Get all JSON files from chunk output directory\n",
    "    files = sorted([f for f in os.listdir(INPUT_DIR) if f.endswith(\".json\")])\n",
    "\n",
    "    for fname in files:\n",
    "        input_path = INPUT_DIR / fname\n",
    "        output_path = OUTPUT_DIR / fname\n",
    "        process_file(input_path, output_path, tokenizer, model)\n",
    "\n",
    "    print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3c84e",
   "metadata": {},
   "source": [
    "# 3. Vector(Cosine) Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_filtering.py\n",
    "# Step-3\n",
    "# Filtering: 1) Auto-filter cosine=1.0, 2) Calculate gap threshold for remaining and filter score < threshold\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model path\n",
    "BGE_EMBED_MODEL = \"BAAI/bge-base-en-v1.5\"  # Use Hugging Face model name or local path\n",
    "\n",
    "# Path settings (matching chunk.py and bert.py structure)\n",
    "BASE_DIR = Path(\"input Low Intensity Factoid Conflict Dataset Folder Path\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"Semantic_Filtered\"  # Input from bert.py output\n",
    "OUTPUT_DIR = DATA_DIR / \"Cosine_Filtered\"  # Output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load Embedding Model\n",
    "# ============================================================\n",
    "def load_embedding_model():\n",
    "    \"\"\"Load BGE embedding model for cosine similarity computation.\"\"\"\n",
    "    print(f\"Loading BGE Embedding Model: {BGE_EMBED_MODEL}\")\n",
    "    model = SentenceTransformer(BGE_EMBED_MODEL)\n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compute Cosine Similarity (per event)\n",
    "# ============================================================\n",
    "def compute_cosine_similarity_event(model, pairs):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity for all sentence pairs in an event.\n",
    "    \n",
    "    Args:\n",
    "        model: SentenceTransformer model\n",
    "        pairs: List of sentence pair dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        list: Cosine similarity scores\n",
    "    \"\"\"\n",
    "    sentences1 = [p[\"sentence_1\"][\"text\"] for p in pairs]\n",
    "    sentences2 = [p[\"sentence_2\"][\"text\"] for p in pairs]\n",
    "\n",
    "    cosine_scores = []\n",
    "    batch_size = 32\n",
    "\n",
    "    for start in range(0, len(pairs), batch_size):\n",
    "        end = min(start + batch_size, len(pairs))\n",
    "\n",
    "        emb1 = model.encode(sentences1[start:end], convert_to_tensor=True)\n",
    "        emb2 = model.encode(sentences2[start:end], convert_to_tensor=True)\n",
    "\n",
    "        batch_scores = util.cos_sim(emb1, emb2).diagonal().cpu().tolist()\n",
    "        cosine_scores.extend(batch_scores)\n",
    "\n",
    "    return cosine_scores\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Modified Cosine Threshold (excluding 1.0 values)\n",
    "# ============================================================\n",
    "def cosine_gap_threshold_excluding_ones(scores):\n",
    "    \"\"\"\n",
    "    Calculate gap-based threshold excluding cosine=1.0 scores.\n",
    "    (Using the largest gap approach)\n",
    "    \n",
    "    Args:\n",
    "        scores: List of cosine similarity scores\n",
    "    \n",
    "    Returns:\n",
    "        float: Threshold value\n",
    "    \"\"\"\n",
    "    # Exclude 1.0 values (consider 0.9999+ as identical)\n",
    "    filtered_scores = [s for s in scores if s < 0.9999]\n",
    "\n",
    "    # If less than 2 scores remain, no meaningful threshold\n",
    "    if len(filtered_scores) < 2:\n",
    "        return 0.9999\n",
    "\n",
    "    # Sort in descending order\n",
    "    sorted_vals = sorted(filtered_scores, reverse=True)\n",
    "\n",
    "    # Calculate gaps between adjacent scores\n",
    "    diffs = np.diff(sorted_vals)\n",
    "\n",
    "    # Find the index of the largest gap\n",
    "    idx = np.argmin(diffs)\n",
    "\n",
    "    # Threshold is the midpoint of this gap\n",
    "    threshold = (sorted_vals[idx] + sorted_vals[idx + 1]) / 2\n",
    "\n",
    "    return threshold\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Extract Event ID from Filename\n",
    "# ============================================================\n",
    "def extract_event_id(filename):\n",
    "    \"\"\"Extract event ID from filename for sorting.\"\"\"\n",
    "    try:\n",
    "        return int(filename.stem.replace(\"event_\", \"\"))\n",
    "    except:\n",
    "        return 999999999\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Process Single File\n",
    "# ============================================================\n",
    "def process_file(input_path, output_path, model):\n",
    "    \"\"\"\n",
    "    Process a single event file for cosine similarity filtering.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input JSON file\n",
    "        output_path: Path to output JSON file\n",
    "        model: SentenceTransformer model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics about the processing\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        pairs = json.load(f)\n",
    "\n",
    "    # 1) Compute cosine similarity\n",
    "    cosine_scores = compute_cosine_similarity_event(model, pairs)\n",
    "\n",
    "    # 2) Calculate modified threshold (excluding 1.0)\n",
    "    threshold = cosine_gap_threshold_excluding_ones(cosine_scores)\n",
    "    \n",
    "    # File-level statistics\n",
    "    file_perfect = 0\n",
    "    file_threshold_filtered = 0\n",
    "\n",
    "    # 3) Apply modified filtering\n",
    "    for p, score in zip(pairs, cosine_scores):\n",
    "        p[\"cosine_score\"] = float(score)\n",
    "        \n",
    "        # Step 1: Auto-filter if cosine=1.0\n",
    "        if score >= 0.9999:\n",
    "            p[\"cosine_filtered\"] = 1\n",
    "            file_perfect += 1\n",
    "        # Step 2: Filter if score < threshold\n",
    "        elif score < threshold:\n",
    "            p[\"cosine_filtered\"] = 1\n",
    "            file_threshold_filtered += 1\n",
    "        # Step 3: Survive if score >= threshold\n",
    "        else:\n",
    "            p[\"cosine_filtered\"] = 0\n",
    "\n",
    "    # 4) Save file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(pairs, g, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return {\n",
    "        \"perfect_matches\": file_perfect,\n",
    "        \"threshold_filtered\": file_threshold_filtered,\n",
    "        \"threshold\": threshold\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model = load_embedding_model()\n",
    "\n",
    "    # Get all JSON files from semantic filtered directory\n",
    "    files = sorted(\n",
    "        [f for f in INPUT_DIR.iterdir() if f.suffix == \".json\"],\n",
    "        key=lambda x: extract_event_id(x)\n",
    "    )\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No JSON files found in {INPUT_DIR}\")\n",
    "        exit(1)\n",
    "\n",
    "    perfect_match_count = 0  # Pairs filtered by cosine≈1.0\n",
    "    threshold_filtered_count = 0  # Pairs filtered by threshold\n",
    "\n",
    "    print(f\"\\nTotal event files: {len(files)}\\n\")\n",
    "\n",
    "    pbar = tqdm(files, desc=\"Processing events\", ncols=120)\n",
    "\n",
    "    for input_file in pbar:\n",
    "        output_file = OUTPUT_DIR / input_file.name\n",
    "        \n",
    "        # Update progress bar with current event\n",
    "        pbar.set_postfix({\n",
    "            \"event\": input_file.stem\n",
    "        })\n",
    "\n",
    "        # Process file\n",
    "        stats = process_file(input_file, output_file, model)\n",
    "        \n",
    "        # Update global statistics\n",
    "        perfect_match_count += stats[\"perfect_matches\"]\n",
    "        threshold_filtered_count += stats[\"threshold_filtered\"]\n",
    "\n",
    "    # ============================================================\n",
    "    # FINAL REPORT\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" \" * 20 + \"FINAL REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nFiltering Statistics:\")\n",
    "    print(f\"- Perfect matches (cosine≈1.0) filtered: {perfect_match_count} pairs\")\n",
    "    print(f\"- Threshold-based filtered: {threshold_filtered_count} pairs\")\n",
    "    print(f\"- Total filtered: {perfect_match_count + threshold_filtered_count} pairs\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036700ee",
   "metadata": {},
   "source": [
    "# 4. Sentence Conflict Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_conflict_detection.py\n",
    "# Step-4: Conflict Detection using GPT\n",
    "# Only verify conflict candidates (semantic_filtered=0 AND cosine_filtered=0) with GPT\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==========================================\n",
    "# CONFIG\n",
    "# ==========================================\n",
    "client = OpenAI(api_key=\"your-api-key-here\")  # Replace with your API key\n",
    "\n",
    "# Path settings (matching previous scripts)\n",
    "BASE_DIR = Path(\"input Low Intensity Factoid Conflict Dataset Folder Path\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"Cosine_Filtered\"  # Input from cosine.py output\n",
    "OUTPUT_DIR = DATA_DIR / \"GPT_Detected\"  # Output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "# MODEL_NAME = \"gpt-4o\"\n",
    "\n",
    "# ==========================================\n",
    "# GPT Prompt\n",
    "# ==========================================\n",
    "def build_prompt_zero_shot(s1, s2):\n",
    "    return f\"\"\"You are a conflict detector for sentence pairs.\n",
    "\n",
    "TASK: Decide if the two sentences make mutually incompatible claims about the same single fact.\n",
    "\n",
    "CONFLICT = 1 when:\n",
    "- For the same event/entity/measure, they give different numbers/dates/times/locations/agents; or\n",
    "- One asserts occurrence while the other denies it; or\n",
    "- Core outcome for the same event is incompatible (e.g., winner/responsible party/casualty count).\n",
    "\n",
    "NO CONFLICT = 0 when:\n",
    "- They discuss different facts/events or different scopes (subset/superset) without contradiction;\n",
    "- Quantifiers/hedges explain the difference (“at least/around/at most/estimated/reported” vs an exact value);\n",
    "- A plausible timepoint/update difference explains it;\n",
    "- Modal/hedged statements don’t directly contradict a categorical claim.\n",
    "\n",
    "Rules:\n",
    "- Use only the two sentences; no outside knowledge.\n",
    "- Normalize numbers/dates/units; treat synonyms/pronouns as the same referent when clear.\n",
    "- If uncertain they refer to the same fact, output 0.\n",
    "- Output only one character with no explanation: 1 for conflict, 0 otherwise.\n",
    "\n",
    "Sentence A: \"{s1}\"\n",
    "Sentence B: \"{s2}\"\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# GPT Request Function\n",
    "# ==========================================\n",
    "def ask_gpt_for_conflict(s1, s2):\n",
    "    \"\"\"\n",
    "    Request GPT to determine if two sentences conflict.\n",
    "    \n",
    "    Args:\n",
    "        s1: First sentence text\n",
    "        s2: Second sentence text\n",
    "    \n",
    "    Returns:\n",
    "        int: 1 if conflict detected, 0 otherwise\n",
    "    \"\"\"\n",
    "    prompt = build_prompt_zero_shot(s1, s2)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=1\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return 1 if answer == \"1\" else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"GPT API Error: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Process Event File\n",
    "# ==========================================\n",
    "def process_event_file(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process a single event file for GPT conflict detection.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input JSON file\n",
    "        output_path: Path to output JSON file\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of conflicts detected\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        pairs = json.load(f)\n",
    "\n",
    "    # Reset labels\n",
    "    for p in pairs:\n",
    "        p[\"conflict_label\"] = 0\n",
    "\n",
    "    # Filter GPT target pairs (survived both semantic and cosine filtering)\n",
    "    gpt_targets = [\n",
    "        p for p in pairs\n",
    "        if p.get(\"semantic_filtered\") == 0 and p.get(\"cosine_filtered\") == 0\n",
    "    ]\n",
    "\n",
    "    # Perform GPT detection\n",
    "    conflicts_detected = 0\n",
    "\n",
    "    print(f\"Processing {len(gpt_targets)} candidate pairs with GPT...\")\n",
    "    \n",
    "    for p in tqdm(gpt_targets, desc=f\"GPT Detection ({input_path.name})\", ncols=120):\n",
    "        s1 = p[\"sentence_1\"][\"text\"]\n",
    "        s2 = p[\"sentence_2\"][\"text\"]\n",
    "\n",
    "        label = ask_gpt_for_conflict(s1, s2)\n",
    "        p[\"conflict_label\"] = label\n",
    "\n",
    "        if label == 1:\n",
    "            conflicts_detected += 1\n",
    "\n",
    "    # Save file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return conflicts_detected\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Extract Event ID for Sorting\n",
    "# ==========================================\n",
    "def extract_event_id(filename):\n",
    "    \"\"\"Extract event ID from filename for sorting.\"\"\"\n",
    "    try:\n",
    "        return int(filename.stem.replace(\"event_\", \"\"))\n",
    "    except:\n",
    "        return 999999999\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# MAIN\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Get all JSON files from cosine filtered directory\n",
    "    files = sorted(\n",
    "        [f for f in INPUT_DIR.iterdir() if f.suffix == \".json\"],\n",
    "        key=lambda x: extract_event_id(x)\n",
    "    )\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No JSON files found in {INPUT_DIR}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"Found {len(files)} event files\")\n",
    "\n",
    "    total_conflicts = 0\n",
    "\n",
    "    for input_file in files:\n",
    "        output_file = OUTPUT_DIR / input_file.name\n",
    "        conflicts_detected = process_event_file(input_file, output_file)\n",
    "        total_conflicts += conflicts_detected\n",
    "\n",
    "        print(f\"Event {input_file.name}: {conflicts_detected} conflicts detected\")\n",
    "\n",
    "    # =============================\n",
    "    # Final Statistics\n",
    "    # =============================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" \" * 20 + \"FINAL REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total events processed: {len(files)}\")\n",
    "    print(f\"Total conflicts detected: {total_conflicts}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3de588",
   "metadata": {},
   "source": [
    "# 5. Document Conflict Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ff59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_conflict_detection.py\n",
    "# Document-level conflict detection using filtered sentence pairs\n",
    "\n",
    "import os, json, random, time, re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# ===========================\n",
    "# Basic Settings\n",
    "# ===========================\n",
    "DATA_PATH = Path(\"input Low Intensity Factoid Conflict Dataset Folder Path\") / \"data\" / \"Low_Intensity_Factoid_Conflict.json\"\n",
    "CHUNK_DIR = Path(\"input Low Intensity Factoid Conflict Dataset Folder Path\") / \"data\" / \"GPT_Detected\"\n",
    "\n",
    "SAVE_DIR = Path(\"input Low Intensity Factoid Conflict Dataset Folder Path\") / \"results\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "client = OpenAI(api_key=\"your-api-key-here\")  # Replace with your API key\n",
    "MODELS = [\"gpt-3.5-turbo\"]\n",
    "GEN_PARAMS = dict(temperature=0.0, max_tokens=512)\n",
    "DOC_COUNT = 5\n",
    "\n",
    "# ===========================\n",
    "# Controlled Random Seeds\n",
    "# ===========================\n",
    "MASTER_SEED = 42\n",
    "SAMPLE_ORDER_SEED = 100\n",
    "ID_SHUFFLE_SEED_BASE = 1000\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Load Candidate Pairs\n",
    "# ================================================================\n",
    "def load_candidate_pairs(event_id):\n",
    "    \"\"\"\n",
    "    Load conflict candidate pairs from GPT detection results.\n",
    "    \n",
    "    Args:\n",
    "        event_id: Event identifier\n",
    "    \n",
    "    Returns:\n",
    "        list: Candidate pairs marked as conflicts by GPT\n",
    "    \"\"\"\n",
    "    path = CHUNK_DIR / f\"event_{event_id}.json\"\n",
    "    if not path.exists():\n",
    "        return []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        pairs = json.load(f)\n",
    "\n",
    "    cands = []\n",
    "    for p in pairs:\n",
    "        if p.get(\"conflict_label\") == 1:\n",
    "            cands.append({\n",
    "                \"s1\": p[\"sentence_1\"][\"text\"],\n",
    "                \"s2\": p[\"sentence_2\"][\"text\"],\n",
    "                \"doc_id_1\": p[\"sentence_1\"][\"doc_id\"],\n",
    "                \"doc_id_2\": p[\"sentence_2\"][\"doc_id\"],\n",
    "            })\n",
    "    return cands\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Document Formatting\n",
    "# ================================================================\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format documents with doc IDs.\"\"\"\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[Doc {i}] ({tag}, id={doc_id}): {txt}\"\n",
    "        for i, (tag, txt, doc_id) in enumerate(docs, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "def map_candidates_to_indices(cands, doc_id_to_idx):\n",
    "    \"\"\"Map candidate pairs to document indices.\"\"\"\n",
    "    used, dropped = [], []\n",
    "    for c in cands:\n",
    "        i1 = doc_id_to_idx.get(c[\"doc_id_1\"])\n",
    "        i2 = doc_id_to_idx.get(c[\"doc_id_2\"])\n",
    "        if not i1 or not i2:\n",
    "            dropped.append(c)\n",
    "            continue\n",
    "        used.append({\n",
    "            \"s1\": c[\"s1\"], \"s2\": c[\"s2\"],\n",
    "            \"i1\": i1, \"i2\": i2\n",
    "        })\n",
    "    return used, dropped\n",
    "\n",
    "\n",
    "def format_focus_pairs(mapped):\n",
    "    \"\"\"Format mapped candidate pairs for GPT prompt.\"\"\"\n",
    "    if not mapped:\n",
    "        return \"\"\n",
    "    out = []\n",
    "    for i, c in enumerate(mapped, 1):\n",
    "        out.append(\n",
    "            f\"[Pair {i}]\\n\"\n",
    "            f\"Sentence A (Doc {c['i1']}): {c['s1']}\\n\"\n",
    "            f\"Sentence B (Doc {c['i2']}): {c['s2']}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Prompts\n",
    "# ================================================================\n",
    "PROMPT_BASE = \"\"\"\n",
    "You are an expert factual conflict detector.\n",
    "\n",
    "You will be given 5 documents (Doc 1..5). Some may describe the SAME event; others may be unrelated.\n",
    "\n",
    "Your task:\n",
    "1) Determine whether any documents contain factual contradictions about the SAME event or entity.\n",
    "2) Translation/paraphrase differences are NOT contradictions.\n",
    "3) Only explicit factual disagreements count.\n",
    "\n",
    "Analyze the following documents:\n",
    "{docs}\n",
    "\n",
    "Output ONLY valid JSON:\n",
    "{{\n",
    "  \"conflict_exists\": true/false,\n",
    "  \"conflicting_docs\": [integers],\n",
    "  \"reason\": \"one concise sentence\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "PROMPT_WITH_HINTS = \"\"\"\n",
    "You are an expert factual conflict detector.\n",
    "\n",
    "You will be given 5 documents (Doc 1..5). Some may describe the SAME event; others may be unrelated.\n",
    "\n",
    "Your task:\n",
    "1) Determine whether any documents contain factual contradictions about the SAME event or entity.\n",
    "2) Translation/paraphrase differences are NOT contradictions.\n",
    "3) Only explicit factual disagreements count.\n",
    "\n",
    "Analyze the following documents:\n",
    "{docs}\n",
    "\n",
    "IMPORTANT: Some sentence pairs are provided below for reference only. These may or may not indicate actual conflicts. \n",
    "You MUST independently analyze ALL documents and make your own judgment.\n",
    "\n",
    "{focus}\n",
    "\n",
    "Output ONLY valid JSON:\n",
    "{{\n",
    "  \"conflict_exists\": true/false,\n",
    "  \"conflicting_docs\": [integers],\n",
    "  \"reason\": \"one concise sentence\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Utility Functions\n",
    "# ================================================================\n",
    "def parse_json(text):\n",
    "    \"\"\"Parse JSON from GPT response, handling various formats.\"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        text = re.sub(r\"^```(json)?|```$\", \"\", text, flags=re.DOTALL).strip()\n",
    "    f, l = text.find(\"{\"), text.rfind(\"}\")\n",
    "    if f != -1 and l != -1:\n",
    "        try: return json.loads(text[f:l+1])\n",
    "        except: return {}\n",
    "    return {}\n",
    "\n",
    "\n",
    "def to_bool(x):\n",
    "    \"\"\"Convert various types to boolean.\"\"\"\n",
    "    if isinstance(x, bool): return x\n",
    "    if isinstance(x, str): return x.strip().lower() in {\"true\", \"1\", \"yes\"}\n",
    "    return False\n",
    "\n",
    "\n",
    "def intify_list(xs):\n",
    "    \"\"\"Extract and validate document indices from various formats.\"\"\"\n",
    "    if xs is None: return []\n",
    "    if isinstance(xs, (int,str)): xs = [xs]\n",
    "    out=[]\n",
    "    for v in xs:\n",
    "        if isinstance(v,int): k=v\n",
    "        else:\n",
    "            m=re.search(r\"\\d+\", str(v))\n",
    "            if not m: continue\n",
    "            k=int(m.group())\n",
    "        if 1<=k<=DOC_COUNT: out.append(k)\n",
    "    return sorted(set(out))\n",
    "\n",
    "\n",
    "def call_gpt(model, prompt):\n",
    "    \"\"\"Call GPT API with retry logic.\"\"\"\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\", \"content\":\"Return ONLY valid JSON.\"},\n",
    "                    {\"role\":\"user\", \"content\":prompt}\n",
    "                ],\n",
    "                **GEN_PARAMS\n",
    "            )\n",
    "            return resp.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            time.sleep(3)\n",
    "    return \"{}\"\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Build Document Sample\n",
    "# ================================================================\n",
    "def build_document_sample(item):\n",
    "    \"\"\"\n",
    "    Build document sample from event data.\n",
    "    \n",
    "    Args:\n",
    "        item: Event data item\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (docs list, doc_id_map)\n",
    "    \"\"\"\n",
    "    n = item[\"news\"]\n",
    "    docs = [\n",
    "        (\"news1\", n[\"news1\"][\"article\"], \"news1\"),\n",
    "        (\"conflict_news\", n[\"conflict_news\"][\"article\"], \"conflict_news\"),\n",
    "        (\"other1\", n[\"other_news1\"][\"article\"], \"other_news1\"),\n",
    "        (\"other2\", n[\"other_news2\"][\"article\"], \"other_news2\"),\n",
    "        (\"other3\", n[\"other_news3\"][\"article\"], \"other_news3\"),\n",
    "    ]\n",
    "    \n",
    "    # Fixed seed per ID for reproducible shuffle\n",
    "    seed = ID_SHUFFLE_SEED_BASE + hash(item[\"id\"]) % 10000\n",
    "    random.seed(seed)\n",
    "    random.shuffle(docs)\n",
    "\n",
    "    doc_id_map = {doc_id: i for i, (tag, _, doc_id) in enumerate(docs, 1)}\n",
    "    \n",
    "    return docs, doc_id_map\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Process Single Event\n",
    "# ================================================================\n",
    "def process_event(model, item):\n",
    "    \"\"\"\n",
    "    Process a single event for conflict detection.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name\n",
    "        item: Event data item\n",
    "    \n",
    "    Returns:\n",
    "        dict: Detection result\n",
    "    \"\"\"\n",
    "    docs, doc_id_map = build_document_sample(item)\n",
    "    \n",
    "    # Load candidate pairs from pipeline\n",
    "    raw_pairs = load_candidate_pairs(item[\"id\"])\n",
    "    mapped_pairs, _ = map_candidates_to_indices(raw_pairs, doc_id_map)\n",
    "    \n",
    "    # Build prompt\n",
    "    docs_text = format_docs(docs)\n",
    "    \n",
    "    if len(mapped_pairs) >= 1:\n",
    "        focus_text = format_focus_pairs(mapped_pairs)\n",
    "        prompt = PROMPT_WITH_HINTS.format(docs=docs_text, focus=focus_text)\n",
    "    else:\n",
    "        prompt = PROMPT_BASE.format(docs=docs_text)\n",
    "    \n",
    "    # Call GPT\n",
    "    raw_out = call_gpt(model, prompt)\n",
    "    parsed = parse_json(raw_out)\n",
    "    \n",
    "    pred_exists = to_bool(parsed.get(\"conflict_exists\"))\n",
    "    pred_docs = intify_list(parsed.get(\"conflicting_docs\", []))\n",
    "    reason = parsed.get(\"reason\", \"\")\n",
    "    \n",
    "    return {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"model\": model,\n",
    "        \"doc_order\": [tag for tag, _, _ in docs],\n",
    "        \"candidate_pairs_used\": len(mapped_pairs),\n",
    "        \"conflict_exists\": pred_exists,\n",
    "        \"conflicting_docs\": pred_docs,\n",
    "        \"reason\": reason,\n",
    "        \"raw_output\": raw_out\n",
    "    }\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# MAIN\n",
    "# ================================================================\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict): \n",
    "        data = [data]\n",
    "\n",
    "    print(f\"Loaded {len(data)} events\")\n",
    "\n",
    "    for model in MODELS:\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# MODEL: {model}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        for item in tqdm(data, desc=f\"Processing events - {model}\"):\n",
    "            result = process_event(model, item)\n",
    "            results.append(result)\n",
    "        \n",
    "        # Save results\n",
    "        output_path = SAVE_DIR / f\"conflict_detection_{model.replace('-', '_')}.json\"\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Summary\n",
    "        conflicts_found = sum(1 for r in results if r[\"conflict_exists\"])\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTS - {model}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total events processed: {len(results)}\")\n",
    "        print(f\"Events with conflicts detected: {conflicts_found}\")\n",
    "        print(f\"Results saved → {output_path}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
